{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":53569,"databundleVersionId":5834979,"sourceType":"competition"}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>MLP Project T2 2023</h1>","metadata":{"execution":{"iopub.execute_input":"2023-08-07T13:51:45.474731Z","iopub.status.busy":"2023-08-07T13:51:45.473957Z","iopub.status.idle":"2023-08-07T13:51:48.427496Z","shell.execute_reply":"2023-08-07T13:51:48.425446Z","shell.execute_reply.started":"2023-08-07T13:51:45.474689Z"},"trusted":true}},{"cell_type":"markdown","source":"Load csv data and convert sentiment column to 0/1 ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# load data\nmovies = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv')\ntrain = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv')\ntest = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv')\n\n# shape before dropping duplicates\nprint(movies.shape)\n\n# drop duplicates by id from movies\nmovies = movies.drop_duplicates(keep='first', subset=['movieid'], inplace=False)\n\n# shape after dropping duplicates\nprint(movies.shape)\n\n# convert sentiment to boolean\ntrain['sentiment'] = train['sentiment'].apply(lambda sentiment : 1 if (sentiment == 'POSITIVE') else 0)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:21:22.395522Z","iopub.execute_input":"2023-08-15T15:21:22.395950Z","iopub.status.idle":"2023-08-15T15:21:25.068066Z","shell.execute_reply.started":"2023-08-15T15:21:22.395914Z","shell.execute_reply":"2023-08-15T15:21:25.066952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge the train and test data on movies data using below parameter\n1. how = 'left' - to result df has same number of rows as train/test\n2. on = 'movieid' - to merge dfs on the basis of movieid column","metadata":{}},{"cell_type":"code","source":"# shape before merge\nprint('train', train.shape)\nprint('test', test.shape)\n\n# merge movies to train and test\ntrain = train.merge(movies, 'left', 'movieid')\ntest = test.merge(movies, 'left', 'movieid')\n\n# shape after merge\nprint('train', train.shape)\nprint('test', test.shape)\n\n# columns of data\ntrain.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:21:25.070002Z","iopub.execute_input":"2023-08-15T15:21:25.070640Z","iopub.status.idle":"2023-08-15T15:21:25.462912Z","shell.execute_reply.started":"2023-08-15T15:21:25.070603Z","shell.execute_reply":"2023-08-15T15:21:25.461580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show info of columns in train dataset","metadata":{}},{"cell_type":"code","source":"# check information of train dataset (column type and number of non-null points)\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:05:17.297720Z","iopub.execute_input":"2023-08-14T04:05:17.298154Z","iopub.status.idle":"2023-08-14T04:05:18.038200Z","shell.execute_reply.started":"2023-08-14T04:05:17.298123Z","shell.execute_reply":"2023-08-14T04:05:18.036778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot a seaborn pairplot and correlation heatmap between numerical columns </br>\n['audienceScore','runtimeMinutes','sentiment']","metadata":{}},{"cell_type":"code","source":"import seaborn\nseaborn.set_style(\"white\")\n\n# pairplot\nnumerical_columns = ['audienceScore','runtimeMinutes','sentiment']\nseaborn.pairplot(train, hue='sentiment', x_vars=numerical_columns, y_vars=numerical_columns)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:05:20.781666Z","iopub.execute_input":"2023-08-14T04:05:20.782104Z","iopub.status.idle":"2023-08-14T04:06:51.514107Z","shell.execute_reply.started":"2023-08-14T04:05:20.782072Z","shell.execute_reply":"2023-08-14T04:06:51.512660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seaborn.heatmap(train[numerical_columns].corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:06:51.516910Z","iopub.execute_input":"2023-08-14T04:06:51.517397Z","iopub.status.idle":"2023-08-14T04:06:51.934809Z","shell.execute_reply.started":"2023-08-14T04:06:51.517358Z","shell.execute_reply":"2023-08-14T04:06:51.933374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess string columns for movie name and director name using one hot encoder\nOHE parameters - \n1. handle_unknown = 'ignore' - to ignore nan values\n2. sparse = True - to get sparse data in return \n3. dtype = int - to ensure integer datatype of result columns","metadata":{}},{"cell_type":"code","source":"# create one-hot-encoders\nfrom sklearn.preprocessing import OneHotEncoder\n\nmovieids = np.concatenate([train.movieid, test.movieid]).reshape(-1,1)\ndirectors = np.concatenate([train.director, test.director]).reshape(-1,1)\n\nmovieid_ohe = OneHotEncoder(handle_unknown='ignore', sparse=True, dtype=int)\ndirector_ohe = OneHotEncoder(handle_unknown='ignore', sparse=True, dtype=int)\n\nmovieid_ohe.fit(movieids)\ndirector_ohe.fit(directors)\n\n# One-hot-encode train.movieid and train.director\ntrain_movieid = movieid_ohe.transform(train.movieid.values.reshape(-1,1))\ntest_movieid = movieid_ohe.transform(test.movieid.values.reshape(-1,1))\ntrain_director = director_ohe.transform(train.director.values.reshape(-1,1))\ntest_director = director_ohe.transform(test.director.values.reshape(-1,1))\n\n# print number of ohe columns of movieid and director\nprint('movieid - ', train_movieid.shape[1])\nprint('director - ', train_director.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:06:51.936567Z","iopub.execute_input":"2023-08-14T04:06:51.936915Z","iopub.status.idle":"2023-08-14T04:06:52.404035Z","shell.execute_reply.started":"2023-08-14T04:06:51.936885Z","shell.execute_reply":"2023-08-14T04:06:52.402637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess 'reviewText' column by lowering text and keeping only alphabetical characters and space </br>\nReplace nan with empty review text","metadata":{}},{"cell_type":"code","source":"# clean reviewText by keeping only alphabets and space\nkeepAlhaNum = re.compile('[^a-z\\s]+')\n\ndef process_text(text):\n    if text is np.nan:\n        return ''\n    return re.sub(keepAlhaNum, '', text.lower())\n    \ntrain['reviewText'] = train['reviewText'].apply(process_text)\ntest['reviewText'] = test['reviewText'].apply(process_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:22:20.184757Z","iopub.execute_input":"2023-08-15T15:22:20.185177Z","iopub.status.idle":"2023-08-15T15:22:21.563664Z","shell.execute_reply.started":"2023-08-15T15:22:20.185147Z","shell.execute_reply":"2023-08-15T15:22:21.562829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vecotrize 'reviewText' column with count-vectorizer with english stopwords","metadata":{}},{"cell_type":"code","source":"# count vectorize reviewText\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(stop_words='english')\n\ntrain_words_matrix = count_vect.fit_transform(train['reviewText'])\ntrain_review_count = pd.DataFrame.sparse.from_spmatrix(train_words_matrix, columns=count_vect.get_feature_names_out())\n\ntest_words_matrix = count_vect.transform(test['reviewText'])\ntest_review_count = pd.DataFrame.sparse.from_spmatrix(test_words_matrix, columns=count_vect.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:06:54.710862Z","iopub.execute_input":"2023-08-14T04:06:54.711449Z","iopub.status.idle":"2023-08-14T04:07:06.224859Z","shell.execute_reply.started":"2023-08-14T04:06:54.711413Z","shell.execute_reply":"2023-08-14T04:07:06.223503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vecotrize 'reviewText' column with tfidf-vectorizer without stopwords","metadata":{}},{"cell_type":"code","source":"# tfidf vectorize reviewText\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer()\n\ntrain_words_matrix = tfidf_vect.fit_transform(train['reviewText'])\ntrain_review_tfidf = pd.DataFrame.sparse.from_spmatrix(train_words_matrix, columns=tfidf_vect.get_feature_names_out())\n\ntest_words_matrix = tfidf_vect.transform(test['reviewText'])\ntest_review_tfidf = pd.DataFrame.sparse.from_spmatrix(test_words_matrix, columns=tfidf_vect.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:06.226683Z","iopub.execute_input":"2023-08-14T04:07:06.227126Z","iopub.status.idle":"2023-08-14T04:07:18.547941Z","shell.execute_reply.started":"2023-08-14T04:07:06.227084Z","shell.execute_reply":"2023-08-14T04:07:18.546537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scale audiencescore using MinMax scaler and use sparse to ensure result matrix is sparse","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom scipy import sparse\n\nscaler = MinMaxScaler()\n\naudienceScores = np.concatenate([train.audienceScore, test.audienceScore]).reshape(-1,1)\nscaler.fit(audienceScores)\n\ntrain_audience_score = sparse.csr_matrix(np.nan_to_num(scaler.transform(train.audienceScore.values.reshape(-1,1)), nan=0, posinf=1, neginf=0))\ntest_audience_score = sparse.csr_matrix(np.nan_to_num(scaler.transform(test.audienceScore.values.reshape(-1,1)), nan=0, posinf=1, neginf=0))","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:18.549492Z","iopub.execute_input":"2023-08-14T04:07:18.549835Z","iopub.status.idle":"2023-08-14T04:07:18.572906Z","shell.execute_reply.started":"2023-08-14T04:07:18.549803Z","shell.execute_reply":"2023-08-14T04:07:18.572099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concat intermediate resultant sparse matrices","metadata":{}},{"cell_type":"code","source":"def concat_sparse(matrixes):\n    dfs = []\n    cols = 0\n    for index, matrix in enumerate(matrixes):\n        cols += matrix.shape[1]\n        if type(matrix).__module__ == np.__name__:\n            print('numpy data')\n            dfs.append(pd.DataFrame(matrix).add_prefix(f'{index}_'))\n        else:\n            print('df data')\n            dfs.append(pd.DataFrame.sparse.from_spmatrix(matrix).add_prefix(f'{index}_'))\n    return pd.concat(dfs, axis=1)\n\ntrain_data = concat_sparse([train_movieid, train_director, train_audience_score])\ntest_data = concat_sparse([test_movieid, test_director, test_audience_score])\n\nprint(train_data.shape)\nprint(test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:18.574182Z","iopub.execute_input":"2023-08-14T04:07:18.575106Z","iopub.status.idle":"2023-08-14T04:07:22.674438Z","shell.execute_reply.started":"2023-08-14T04:07:18.575060Z","shell.execute_reply":"2023-08-14T04:07:22.672885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we know what we have to do, let's try using Pipelines!\n(we will also add imputer to pipelines)","metadata":{}},{"cell_type":"code","source":"# verify train test df shape\nprint(train.shape, test.shape)\n\n# identified numerical and categorical columns\nnum_cols = ['audienceScore']\ncat_cols = ['movieid', 'director']\n\n# define pipelines using minmax for numerical, ohe for categorical with constant simple imputer for both and tfidf for text column\n\n# import libraries for pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# pipeline for audienceScore column with imputer fill_value = 40\naud_pipeline = Pipeline(steps=[('impute', SimpleImputer(\n    strategy='constant', fill_value=40)), ('minmax', MinMaxScaler())], verbose=True)\n\n# generic pipeline for all categorical columns\ncat_pipeline = Pipeline(steps=[('impute', SimpleImputer(strategy='constant', fill_value='')), (\n    'ohe', OneHotEncoder(handle_unknown='ignore', sparse=True, dtype=int))], verbose=True)\n\n# pipeline to process text column using tfidf\ntex_pipeline = Pipeline(steps=[('tfidf-vectorize', TfidfVectorizer())], verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:22:29.220653Z","iopub.execute_input":"2023-08-15T15:22:29.221077Z","iopub.status.idle":"2023-08-15T15:22:29.229817Z","shell.execute_reply.started":"2023-08-15T15:22:29.221046Z","shell.execute_reply":"2023-08-15T15:22:29.228620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create column transformer using above defined pipelines\n\nnote - problem with tex_pipeline in column transformer, therefore processed seperately and hstacked","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\ncol_transformer = ColumnTransformer(transformers=[('aud_pipeline', aud_pipeline, ['audienceScore']), (\n    'cat_pipeline', cat_pipeline, cat_cols)], remainder='drop', n_jobs=2, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:22:33.532567Z","iopub.execute_input":"2023-08-15T15:22:33.532990Z","iopub.status.idle":"2023-08-15T15:22:33.538642Z","shell.execute_reply.started":"2023-08-15T15:22:33.532955Z","shell.execute_reply":"2023-08-15T15:22:33.537370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now visualise column transformer and pipelines below\n\nnote - diagram is interactive, feel free to click and expand on it","metadata":{}},{"cell_type":"code","source":"from sklearn import set_config\n\nset_config(display='diagram')\ndisplay(col_transformer), display(tex_pipeline)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:23.043176Z","iopub.execute_input":"2023-08-14T04:07:23.043843Z","iopub.status.idle":"2023-08-14T04:07:23.100752Z","shell.execute_reply.started":"2023-08-14T04:07:23.043808Z","shell.execute_reply":"2023-08-14T04:07:23.099832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data into train, test sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    train, train['sentiment'], test_size=0.25, shuffle=True)\n\nfor df in [x_train, x_test, y_train, y_test]:\n    print(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:22:38.494690Z","iopub.execute_input":"2023-08-15T15:22:38.495096Z","iopub.status.idle":"2023-08-15T15:22:38.728859Z","shell.execute_reply.started":"2023-08-15T15:22:38.495065Z","shell.execute_reply":"2023-08-15T15:22:38.727488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess the split train data using defined pipelines and transformer","metadata":{}},{"cell_type":"code","source":"col_transformer.fit(x_train, y_train)\ntex_pipeline.fit(x_train['reviewText'], y_train)\n\nx_train_col = col_transformer.transform(x_train)\nx_test_col = col_transformer.transform(x_test)\n\nx_train_tf = tex_pipeline.transform(x_train['reviewText'])\nx_test_tf = tex_pipeline.transform(x_test['reviewText'])\n\nfrom scipy.sparse import hstack\n\nx_train_res = hstack((x_train_col, x_train_tf))\nx_test_res = hstack((x_test_col, x_test_tf))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:22:40.817625Z","iopub.execute_input":"2023-08-15T15:22:40.818030Z","iopub.status.idle":"2023-08-15T15:22:47.839650Z","shell.execute_reply.started":"2023-08-15T15:22:40.817995Z","shell.execute_reply":"2023-08-15T15:22:47.838588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduce dimensionality of data using SelectKBest\n(useful for models like knn and svc, which are time and resource heavy)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\nk10k = SelectKBest(chi2, k=2500).fit(x_train_res, y_train)\nx_train_10k = k10k.transform(x_train_res)\nx_test_10k = k10k.transform(x_test_res)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:35.969402Z","iopub.execute_input":"2023-08-14T04:07:35.969766Z","iopub.status.idle":"2023-08-14T04:07:36.281940Z","shell.execute_reply.started":"2023-08-14T04:07:35.969726Z","shell.execute_reply":"2023-08-14T04:07:36.280740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define models to train","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nlog = LogisticRegression(random_state=0, verbose=10)\nknn = KNeighborsClassifier()\nsvc = SVC(random_state=0, verbose=10)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:36.283708Z","iopub.execute_input":"2023-08-14T04:07:36.284136Z","iopub.status.idle":"2023-08-14T04:07:36.291180Z","shell.execute_reply.started":"2023-08-14T04:07:36.284098Z","shell.execute_reply":"2023-08-14T04:07:36.290301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find score of individual models","metadata":{}},{"cell_type":"code","source":"log.fit(x_train_res, y_train)\nprint(log.score(x_test_res, y_test))\n\nknn.fit(x_train_10k, y_train)\nprint(knn.score(x_test_10k, y_test))\n\nsvc.fit(x_train_10k, y_train)\nprint(svc.score(x_test_10k, y_test))\n\n# c:\\Users\\1mana\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n# STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\n# Increase the number of iterations (max_iter) or scale the data as shown in:\n#     https://scikit-learn.org/stable/modules/preprocessing.html\n# Please also refer to the documentation for alternative solver options:\n#     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n#   n_iter_i = _check_optimize_result(\n# [Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   21.3s\n# [Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   21.3s\n# 0.822732858196117\n# 0.7436962398623741\n# [LibSVM]0.7871467190956009","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:07:36.292624Z","iopub.execute_input":"2023-08-14T04:07:36.293632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare models graphically","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), sharey=True)\n\ncommon_params = {\n    \"X\": x_train_res,\n    \"y\": y_train,\n    \"score_type\": \"both\",\n    \"n_jobs\": 4,\n    \"line_kw\": {\"marker\": \"o\"},\n    \"std_display_style\": \"fill_between\",\n    \"score_name\": \"Accuracy\",\n}\n\nfor ax_idx, estimator in enumerate([log]):\n    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n    handles, label = ax[ax_idx].get_legend_handles_labels()\n    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n\ncommon_params = {\n    \"X\": x_train_10k,\n    \"y\": y_train,\n    \"score_type\": \"both\",\n    \"n_jobs\": 4,\n    \"line_kw\": {\"marker\": \"o\"},\n    \"std_display_style\": \"fill_between\",\n    \"score_name\": \"Accuracy\",\n}\n\nfor ax_idx, estimator in enumerate([knn, svc]):\n    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n    handles, label = ax[ax_idx].get_legend_handles_labels()\n    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression seems to work best, let's optimise using Grid search cv","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# params = {'C': [0.1, 1, 10], 'tol': [0.001, 0.0001, 0.00001], 'penalty': ['l1','l2'], 'solver': ['liblinear', 'saga'], 'max_iter': [100, 250, 1000]}\n\n# log_search = GridSearchCV(log_opt, params, verbose=10, cv=3, n_jobs=3)\n# log_search.fit(x_train_res, y_train)\n# print(log_search.score(x_test_res, y_test))\n# print(log_search.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use Optimised Logistic Regression on train data to predict test data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_opt = LogisticRegression(random_state=0, verbose=10, max_iter=250, penalty='l2', solver='saga', C=1, tol=0.00001, warm_start=False)\n\nlog_opt.fit(x_train_res, y_train)\nlog_opt.score(x_test_res, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:23:35.123996Z","iopub.execute_input":"2023-08-15T15:23:35.124392Z","iopub.status.idle":"2023-08-15T15:23:46.105857Z","shell.execute_reply.started":"2023-08-15T15:23:35.124364Z","shell.execute_reply":"2023-08-15T15:23:46.104760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_pred = log_opt.predict(x_test_res)\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T15:25:16.556401Z","iopub.execute_input":"2023-08-15T15:25:16.556990Z","iopub.status.idle":"2023-08-15T15:25:16.885144Z","shell.execute_reply.started":"2023-08-15T15:25:16.556957Z","shell.execute_reply":"2023-08-15T15:25:16.884098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare log and log_opt graphically","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n\ncommon_params = {\n    \"X\": x_train_res,\n    \"y\": y_train,\n    \"score_type\": \"both\",\n    \"n_jobs\": 4,\n    \"line_kw\": {\"marker\": \"o\"},\n    \"std_display_style\": \"fill_between\",\n    \"score_name\": \"Accuracy\",\n}\n\nfor ax_idx, estimator in enumerate([log, log_opt]):\n    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n    handles, label = ax[ax_idx].get_legend_handles_labels()\n    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess train and test data using pipelines","metadata":{}},{"cell_type":"code","source":"col_transformer.fit(train, train['sentiment'])\ntex_pipeline.fit(train['reviewText'], train['sentiment'])\n\ntrain_col = col_transformer.transform(train)\ntest_col = col_transformer.transform(test)\n\ntrain_tf = tex_pipeline.transform(train['reviewText'])\ntest_tf = tex_pipeline.transform(test['reviewText'])\n\nfrom scipy.sparse import hstack\n\ntrain_res = hstack((train_col, train_tf))\ntest_res = hstack((test_col, test_tf))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use optimised Logistic Regression to get predictions","metadata":{}},{"cell_type":"code","source":"log_opt.fit(train_res, train['sentiment'])\ntest['sentiment'] = log_opt.predict(test_res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save result to output csv","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame()\noutput['sentiment'] = test['sentiment'].apply(lambda prediction : 'POSITIVE' if prediction == 1 else 'NEGATIVE')\noutput['id'] = test.index\noutput.to_csv('output.csv',index=False, columns=['id', 'sentiment'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---- End of main code ----","metadata":{}},{"cell_type":"markdown","source":"Attempts at Grid Search","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# params = {'C': [0.1, 1, 10], 'tol': [0.001, 0.0001, 0.00001]}\n\n# log_search = GridSearchCV(log_opt, params, verbose=10, cv=3, n_jobs=3)\n# log_search.fit(x_train_res, y_train)\n# print(log_search.score(x_test_res, y_test))\n# print(log_search.best_params_)\n\n# # Fitting 3 folds for each of 9 candidates, totalling 27 fits\n# # convergence after 70 epochs took 8 seconds\n# # 0.8212582944212338\n# # {'C': 1, 'tol': 1e-05}\n# # [Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    7.5s\n# # [Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    7.5s","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# grid_params = [{'clf': log, 'params': {'penalty': ['l1','l2'], 'solver': ['liblinear', 'saga'], 'max_iter': [1, 10, 100, 250, 1000], 'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}}, {'clf': knn, 'params': {}}]\n\n# params = {'penalty': ['l1','l2'], 'solver': ['liblinear', 'saga'], 'max_iter': [100, 250, 1000]}\n\n# clf = GridSearchCV(log, params, verbose=10, cv=5, n_jobs=4)\n# clf.fit(x_train_tfidf, y_train_tfidf)\n# print(clf.score(x_test_tfidf, y_test_tfidf))\n# print(clf.best_params_)\n# print(clf.refit_time_)\n\n# # Grid search on logistic regression with tfidf vectorized data\n\n# # Fitting 5 folds for each of 12 candidates, totalling 60 fits\n# # params = {'penalty': ['l1','l2'], 'solver': ['liblinear', 'saga'], 'max_iter': [100, 250, 1000]}\n\n# # best params\n# # {'max_iter': 250, 'penalty': 'l2', 'solver': 'saga'}\n\n# # score\n# # 0.8216269353649546","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # count vectorize reviewText\n\n# from sklearn.feature_extraction.text import CountVectorizer\n\n# count_vect = CountVectorizer(stop_words='english')\n\n# train_words_matrix = count_vect.fit_transform(train['reviewText'])\n# train_review_count = pd.DataFrame.sparse.from_spmatrix(train_words_matrix, columns=count_vect.get_feature_names_out())\n\n# test_words_matrix = count_vect.transform(test['reviewText'])\n# test_review_count = pd.DataFrame.sparse.from_spmatrix(test_words_matrix, columns=count_vect.get_feature_names_out())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # best svc model\n\n# from sklearn.svm import SVC\n\n# svc = SVC()\n\n# svc.fit(train_10k, y_train_count)\n# svc.score(test_10k, y_test_count)\n\n# # 0.7970017203244041\n\n# svc.get_params()\n\n# # {'C': 1.0,\n# #  'break_ties': False,\n# #  'cache_size': 200,\n# #  'class_weight': None,\n# #  'coef0': 0.0,\n# #  'decision_function_shape': 'ovr',\n# #  'degree': 3,\n# #  'gamma': 'scale',\n# #  'kernel': 'rbf',\n# #  'max_iter': -1,\n# #  'probability': False,\n# #  'random_state': None,\n# #  'shrinking': True,\n# #  'tol': 0.001,\n# #  'verbose': False}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # attempt at decision tree\n\n# from sklearn.tree import DecisionTreeClassifier\n\n# dtc = DecisionTreeClassifier()\n\n# dtc.fit(x_train_count, y_train_count)\n# dtc.score(x_test_count, y_test_count)\n\n# # 0.7123126075202753","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # attempt at mlp\n\n# from sklearn.neural_network import MLPClassifier\n\n# mlp = MLPClassifier()\n\n# mlp.fit(train_10k, y_train_count)\n# mlp.score(test_10k, y_test_count)\n\n# # 0.7553944458097813\n\n# mlp.get_params()\n\n# # {'activation': 'relu',\n# #  'alpha': 0.0001,\n# #  'batch_size': 'auto',\n# #  'beta_1': 0.9,\n# #  'beta_2': 0.999,\n# #  'early_stopping': False,\n# #  'epsilon': 1e-08,\n# #  'hidden_layer_sizes': (100,),\n# #  'learning_rate': 'constant',\n# #  'learning_rate_init': 0.001,\n# #  'max_fun': 15000,\n# #  'max_iter': 200,\n# #  'momentum': 0.9,\n# #  'n_iter_no_change': 10,\n# #  'nesterovs_momentum': True,\n# #  'power_t': 0.5,\n# #  'random_state': None,\n# #  'shuffle': True,\n# #  'solver': 'adam',\n# #  'tol': 0.0001,\n# #  'validation_fraction': 0.1,\n# #  'verbose': False,\n# #  'warm_start': False}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n\n# from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n\n# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n\n# common_params = {\n#     \"X\": x_train_res,\n#     \"y\": y_train,\n#     \"score_type\": \"both\",\n#     \"n_jobs\": 4,\n#     \"line_kw\": {\"marker\": \"o\"},\n#     \"std_display_style\": \"fill_between\",\n#     \"score_name\": \"Accuracy\",\n# }\n\n# for ax_idx, estimator in enumerate([log, log_opt]):\n#     LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n#     handles, label = ax[ax_idx].get_legend_handles_labels()\n#     ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n#     ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")","metadata":{},"execution_count":null,"outputs":[]}]}